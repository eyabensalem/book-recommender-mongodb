
# ğŸ¯ **Projet Big Data â€“ Pipeline complet (ETL â€¢ API â€¢ Dashboard â€¢ Analytics â€¢ ML)**

# ğŸ“˜ **PrÃ©sentation du projet**

Ce projet met en place un **pipeline Big Data complet** permettant :

* ğŸ“¥ **Ingestion** de donnÃ©es livres (CSV â†’ MongoDB)
* ğŸ§¹ **Nettoyage et transformation** (Quality checks, enrichissement)
* ğŸ“Š **Analyse statistique** + gÃ©nÃ©ration de graphiques
* ğŸ”Œ **API REST FastAPI**
* ğŸ“ˆ **Dashboard interactif Streamlit**
* ğŸ¤– **Machine Learning** (modÃ¨le prÃ©dictif)
* ğŸ“¦ **Export des rÃ©sultats** dans un dossier `artifacts/`


---

# ğŸ—ï¸ **Architecture globale**

```
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚        Dataset CSV        â”‚
                   â”‚      (books_raw.csv)      â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                        ğŸ“¥ Ingestion (Python)
                                   â”‚
                                   â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚        MongoDB Atlas        â”‚
                 â”‚  (stockage + logs ingestion)â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                          ğŸ§¹ Transformation
                                   â”‚
                                   â–¼
                data/books_clean.csv (dataset propre)
                                   â”‚
                                   â–¼
                     ğŸ“Š Analyse & Visualisation
        (top books, top authors, yearly rating, stats globales)
                                   â”‚
                                   â–¼
                       ğŸ¤– Machine Learning
          RandomForest â†’ modÃ¨le.pkl + mÃ©triques dâ€™entraÃ®nement
                                   â”‚
                                   â–¼
                       ğŸ“¦ artifacts/ (outputs)
                                   â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚                    â”‚                â”‚
             â–¼                    â–¼                â–¼
       ğŸ”Œ API FastAPI      ğŸ“ˆ Dashboard Streamlit   CSV/PNG
```

---

# ğŸ“‚ **Structure du projet**

```
TP_bigdata/
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ artifacts/               # Graphiques, CSV, modÃ¨le ML
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ books_raw.csv
â”‚   â””â”€â”€ books_clean.csv
â”‚
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â””â”€â”€ ingest_books.py
â”‚   â”œâ”€â”€ transformation/
â”‚   â”‚   â””â”€â”€ clean_books.py
â”‚   â”œâ”€â”€ analytics/
â”‚   â”‚   â””â”€â”€ analyze_books.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ app.py
â”‚   â””â”€â”€ dashboard/
â”‚       â””â”€â”€ app.py
```

---

# âš™ï¸ **Installation**

### 1. Cloner et activer lâ€™environnement

```bash
git clone https://github.com/eya/Projet_bigdata.git
cd Projet_bigdata
python -m venv .venv
.venv\Scripts\activate
```

### 2. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

---

# 1ï¸âƒ£ **Ingestion des donnÃ©es**

```bash
python src/ingestion/ingest_books.py
```

* Lecture du fichier brut
* Enregistrement dans MongoDB
* Log dâ€™ingestion

---

# 2ï¸âƒ£ **Transformation & Nettoyage**

```bash
python src/transformation/clean_books.py
```

* Nettoyage intelligent des donnÃ©es
* Enrichissement (scores, valeurs dÃ©rivÃ©es)
* Gestion des valeurs manquantes
* Export â†’ `data/books_clean.csv`

---

# 3ï¸âƒ£ **Analyse et visualisation**

```bash
python src/analytics/analyze_books.py
```

GÃ©nÃ¨re automatiquement :

ğŸ“Œ statistiques globales
ğŸ“Œ top livres
ğŸ“Œ top auteurs
ğŸ“Œ Ã©volution du rating par annÃ©e
ğŸ“Œ graphiques (PNG)
ğŸ“Œ CSV d'analyse
ğŸ“Œ modÃ¨le ML â†’ `artifacts/model.pkl`

---

# 4ï¸âƒ£ **API FastAPI**

### Lancer l'API

```bash
uvicorn src.api.app:app --reload --port 8000
```

Endpoints disponibles :

| Endpoint         | Description               |
| ---------------- | ------------------------- |
| `/health`        | VÃ©rifier l'Ã©tat de l'API  |
| `/collections`   | Liste MongoDB             |
| `/top-books`     | Top 10                    |
| `/top-authors`   | Classement auteurs        |
| `/yearly-rating` | Ã‰volution                 |
| `/predict`       | ML : PrÃ©diction du rating |

---

# 5ï¸âƒ£ **Dashboard (Streamlit)**

### Lancer le dashboard :

```bash
streamlit run src/dashboard/app.py
```

FonctionnalitÃ©s :

* ğŸ“Š statistiques globales
* ğŸ† top books / top authors
* ğŸ“ˆ graphiques dÃ©jÃ  gÃ©nÃ©rÃ©s
* ğŸ¤– prÃ©dictions ML
* ğŸ”Œ communication automatique avec lâ€™API

### Configuration optionnelle (`secrets.toml`)

```
~/.streamlit/secrets.toml
```

```toml
API_BASE = "http://127.0.0.1:8000"
```

---

# ğŸ¤– **Machine Learning**

ModÃ¨le utilisÃ© :
â¡ï¸ **RandomForestRegressor**

Objectif :
ğŸ“Œ prÃ©dire le `average_rating` Ã  partir de :

* num_pages
* ratings_count
* text_reviews_count
* popularity_score

RÃ©sultats exportÃ©s dans :

```
artifacts/model.pkl
```

---

# ğŸ§ª **Tests rapides**

### API

[http://127.0.0.1:8000/health](http://127.0.0.1:8000/health)

### Dashboard

[http://localhost:8501](http://localhost:8501)

---
![Dashboard home](images/dashboard1.png)
![Dashboard home](images/dashboard2.png)
![Dashboard home](images/dashboard3.png)
![Dashboard home](images/dashboard4.png)

# ğŸ Conclusion 

Ce projet illustre la mise en place dâ€™un **pipeline Big Data complet**, depuis la collecte des donnÃ©es jusquâ€™Ã  leur exploitation via une API et un dashboard interactif.
Il dÃ©montre :

* la maÃ®trise de l'ingestion et de la gestion de donnÃ©es volumineuses
* lâ€™application de techniques de **data cleaning** et de transformation
* la capacitÃ© Ã  produire des analyses statistiques automatisÃ©es
* la crÃ©ation d'une API performante (FastAPI)
* l'intÃ©gration dans un dashboard professionnel (Streamlit)
* lâ€™entraÃ®nement dâ€™un modÃ¨le Machine Learning intÃ©grÃ© au pipeline

â¡ï¸ Ce projet reflÃ¨te une architecture **rÃ©aliste et opÃ©rationnelle**, similaire Ã  ce qui est utilisÃ© dans les entreprises data-driven.

---

